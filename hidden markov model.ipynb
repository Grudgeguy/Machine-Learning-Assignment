{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:02:08.084456Z",
     "iopub.status.busy": "2023-05-11T07:02:08.083897Z",
     "iopub.status.idle": "2023-05-11T07:02:09.213010Z",
     "shell.execute_reply": "2023-05-11T07:02:09.212098Z",
     "shell.execute_reply.started": "2023-05-11T07:02:08.084420Z"
    }
   },
   "outputs": [],
   "source": [
    "import hmmlearn\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:02:31.722235Z",
     "iopub.status.busy": "2023-05-11T07:02:31.720807Z",
     "iopub.status.idle": "2023-05-11T07:02:32.411075Z",
     "shell.execute_reply": "2023-05-11T07:02:32.410076Z",
     "shell.execute_reply.started": "2023-05-11T07:02:31.722199Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt  # show graph\n",
    "import random\n",
    "\n",
    "#some other libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, \\\n",
    "    f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook we will look at the NER dataset and use it to understand HMM and also construct a POS tagger at the same time.\n",
    "\n",
    "### Data Description:\n",
    "#### sentence: this column donates to which sentence the word belongs\n",
    "#### Word: the word in the sentence\n",
    "#### POS: Associated POS tag for the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:02:32.413610Z",
     "iopub.status.busy": "2023-05-11T07:02:32.412318Z",
     "iopub.status.idle": "2023-05-11T07:02:33.230656Z",
     "shell.execute_reply": "2023-05-11T07:02:33.229720Z",
     "shell.execute_reply.started": "2023-05-11T07:02:32.413572Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"NER.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a total of 47960 sentences, with 1,048,575 words. The entities includes 9 named entity types including person names, locations, organizations, dates, times, percentages and others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:02:58.172580Z",
     "iopub.status.busy": "2023-05-11T07:02:58.172222Z",
     "iopub.status.idle": "2023-05-11T07:02:59.524834Z",
     "shell.execute_reply": "2023-05-11T07:02:59.523938Z",
     "shell.execute_reply.started": "2023-05-11T07:02:58.172551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.fillna(method=\"ffill\")\n",
    "data = data.rename(columns={'Sentence #': 'sentence'})\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:03:10.411627Z",
     "iopub.status.busy": "2023-05-11T07:03:10.411254Z",
     "iopub.status.idle": "2023-05-11T07:03:10.417979Z",
     "shell.execute_reply": "2023-05-11T07:03:10.416872Z",
     "shell.execute_reply.started": "2023-05-11T07:03:10.411599Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_processing(text_column):\n",
    "    # lowercase all text in the column\n",
    "    text_column = text_column.str.lower()\n",
    "\n",
    "    # replacing numbers with NUM token\n",
    "    text_column = text_column.str.replace(r'\\d+', 'NUM')\n",
    "\n",
    "    # removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_column = text_column.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "    return text_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:03:12.662791Z",
     "iopub.status.busy": "2023-05-11T07:03:12.662437Z",
     "iopub.status.idle": "2023-05-11T07:03:15.430103Z",
     "shell.execute_reply": "2023-05-11T07:03:15.429133Z",
     "shell.execute_reply.started": "2023-05-11T07:03:12.662764Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_1896\\3849771148.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  text_column = text_column.str.replace(r'\\d+', 'NUM')\n"
     ]
    }
   ],
   "source": [
    "data_pre_precessed = pre_processing(data.Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:03:16.952448Z",
     "iopub.status.busy": "2023-05-11T07:03:16.951750Z",
     "iopub.status.idle": "2023-05-11T07:03:16.963643Z",
     "shell.execute_reply": "2023-05-11T07:03:16.962682Z",
     "shell.execute_reply.started": "2023-05-11T07:03:16.952415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         thousands\n",
       "1                  \n",
       "2     demonstrators\n",
       "3                  \n",
       "4           marched\n",
       "5                  \n",
       "6            london\n",
       "7                  \n",
       "8           protest\n",
       "9                  \n",
       "10              war\n",
       "11                 \n",
       "12             iraq\n",
       "13                 \n",
       "14           demand\n",
       "15                 \n",
       "16       withdrawal\n",
       "17                 \n",
       "18          british\n",
       "19           troops\n",
       "Name: Word, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pre_precessed.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:03:17.658592Z",
     "iopub.status.busy": "2023-05-11T07:03:17.657733Z",
     "iopub.status.idle": "2023-05-11T07:03:18.119234Z",
     "shell.execute_reply": "2023-05-11T07:03:18.118302Z",
     "shell.execute_reply.started": "2023-05-11T07:03:17.658557Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating new dataframe with preprocessed word as a column\n",
    "data_processed = data\n",
    "data_processed['Word'] = data_pre_precessed\n",
    "\n",
    "#removing the rows where word is empty\n",
    "data_processed = data_processed[(data_processed['Word'] != '') | (data_processed['Word'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:03:18.481537Z",
     "iopub.status.busy": "2023-05-11T07:03:18.481153Z",
     "iopub.status.idle": "2023-05-11T07:03:18.495389Z",
     "shell.execute_reply": "2023-05-11T07:03:18.494436Z",
     "shell.execute_reply.started": "2023-05-11T07:03:18.481505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>british</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>conflict</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>joined</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>protesters</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>carried</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence           Word  POS    Tag\n",
       "0   Sentence: 1      thousands  NNS      O\n",
       "2   Sentence: 1  demonstrators  NNS      O\n",
       "4   Sentence: 1        marched  VBN      O\n",
       "6   Sentence: 1         london  NNP  B-geo\n",
       "8   Sentence: 1        protest   VB      O\n",
       "10  Sentence: 1            war   NN      O\n",
       "12  Sentence: 1           iraq  NNP  B-geo\n",
       "14  Sentence: 1         demand   VB      O\n",
       "16  Sentence: 1     withdrawal   NN      O\n",
       "18  Sentence: 1        british   JJ  B-gpe\n",
       "19  Sentence: 1         troops  NNS      O\n",
       "22  Sentence: 1        country   NN      O\n",
       "23  Sentence: 1              .    .      O\n",
       "24  Sentence: 2       families  NNS      O\n",
       "26  Sentence: 2       soldiers  NNS      O\n",
       "27  Sentence: 2         killed  VBN      O\n",
       "30  Sentence: 2       conflict   NN      O\n",
       "31  Sentence: 2         joined  VBD      O\n",
       "33  Sentence: 2     protesters  NNS      O\n",
       "35  Sentence: 2        carried  VBD      O"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:03:19.564305Z",
     "iopub.status.busy": "2023-05-11T07:03:19.563642Z",
     "iopub.status.idle": "2023-05-11T07:03:19.696664Z",
     "shell.execute_reply": "2023-05-11T07:03:19.695740Z",
     "shell.execute_reply.started": "2023-05-11T07:03:19.564269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 29764)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(set(data.POS.values))  # Unique POS tags in the dataset\n",
    "words = list(set(data.Word.values))  # Unique words in the dataset\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:03:22.030495Z",
     "iopub.status.busy": "2023-05-11T07:03:22.029923Z",
     "iopub.status.idle": "2023-05-11T07:03:22.111443Z",
     "shell.execute_reply": "2023-05-11T07:03:22.110288Z",
     "shell.execute_reply.started": "2023-05-11T07:03:22.030461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29763"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1 = list(set(data_processed.Word.values))  # Unique words in the dataset\n",
    "len(words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have 42 different tags and 29,764 different words, so the HMM that we construct will have the following properties\n",
    "- The hidden states of the this HMM will correspond to the POS tags, so we will have 42 hidden states.\n",
    "- The Observations for this HMM will correspond to the sentences and their words.\n",
    "\n",
    "#### Before constructing the HMM, we will split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:03:26.955683Z",
     "iopub.status.busy": "2023-05-11T07:03:26.954871Z",
     "iopub.status.idle": "2023-05-11T07:03:28.540129Z",
     "shell.execute_reply": "2023-05-11T07:03:28.539162Z",
     "shell.execute_reply.started": "2023-05-11T07:03:26.955645Z"
    }
   },
   "outputs": [],
   "source": [
    "y = data.POS\n",
    "X = data.drop('POS', axis=1)\n",
    "\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.33, random_state=42)\n",
    "train_ix, test_ix = next(gs.split(X, y, groups=data['sentence']))\n",
    "\n",
    "data_train = data.loc[train_ix]\n",
    "data_test = data.loc[test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:03:57.313722Z",
     "iopub.status.busy": "2023-05-11T07:03:57.313338Z",
     "iopub.status.idle": "2023-05-11T07:03:57.325097Z",
     "shell.execute_reply": "2023-05-11T07:03:57.323885Z",
     "shell.execute_reply.started": "2023-05-11T07:03:57.313691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td></td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td></td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence      Word  POS Tag\n",
       "24  Sentence: 2  families  NNS   O\n",
       "25  Sentence: 2             IN   O\n",
       "26  Sentence: 2  soldiers  NNS   O\n",
       "27  Sentence: 2    killed  VBN   O\n",
       "28  Sentence: 2             IN   O"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:03:58.501957Z",
     "iopub.status.busy": "2023-05-11T07:03:58.501606Z",
     "iopub.status.idle": "2023-05-11T07:03:58.514094Z",
     "shell.execute_reply": "2023-05-11T07:03:58.513051Z",
     "shell.execute_reply.started": "2023-05-11T07:03:58.501929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td></td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td></td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence           Word  POS Tag\n",
       "0  Sentence: 1      thousands  NNS   O\n",
       "1  Sentence: 1                  IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1                 VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:03:59.634225Z",
     "iopub.status.busy": "2023-05-11T07:03:59.633839Z",
     "iopub.status.idle": "2023-05-11T07:04:00.699358Z",
     "shell.execute_reply": "2023-05-11T07:04:00.698379Z",
     "shell.execute_reply.started": "2023-05-11T07:03:59.634194Z"
    }
   },
   "outputs": [],
   "source": [
    "#using preprocessed data \n",
    "\n",
    "y1 = data_processed.POS\n",
    "X1 = data_processed.drop('POS', axis=1)\n",
    "data_processed.reset_index(drop=True, inplace=True)\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.33, random_state=42)\n",
    "train_ix1, test_ix1 = next(gs.split(X1, y1, groups=data_processed['sentence']))\n",
    "\n",
    "data_train1 = data_processed.loc[train_ix1]\n",
    "data_test1 = data_processed.loc[test_ix1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:04:01.554634Z",
     "iopub.status.busy": "2023-05-11T07:04:01.553825Z",
     "iopub.status.idle": "2023-05-11T07:04:01.568278Z",
     "shell.execute_reply": "2023-05-11T07:04:01.567296Z",
     "shell.execute_reply.started": "2023-05-11T07:04:01.554589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>conflict</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>joined</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence      Word  POS Tag\n",
       "13  Sentence: 2  families  NNS   O\n",
       "14  Sentence: 2  soldiers  NNS   O\n",
       "15  Sentence: 2    killed  VBN   O\n",
       "16  Sentence: 2  conflict   NN   O\n",
       "17  Sentence: 2    joined  VBD   O"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:04:02.829655Z",
     "iopub.status.busy": "2023-05-11T07:04:02.829113Z",
     "iopub.status.idle": "2023-05-11T07:04:02.840563Z",
     "shell.execute_reply": "2023-05-11T07:04:02.839334Z",
     "shell.execute_reply.started": "2023-05-11T07:04:02.829622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence           Word  POS    Tag\n",
       "0  Sentence: 1      thousands  NNS      O\n",
       "1  Sentence: 1  demonstrators  NNS      O\n",
       "2  Sentence: 1        marched  VBN      O\n",
       "3  Sentence: 1         london  NNP  B-geo\n",
       "4  Sentence: 1        protest   VB      O"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets encode the POS and Words to be used to generate the HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:04:09.105756Z",
     "iopub.status.busy": "2023-05-11T07:04:09.105314Z",
     "iopub.status.idle": "2023-05-11T07:04:09.649145Z",
     "shell.execute_reply": "2023-05-11T07:04:09.648256Z",
     "shell.execute_reply.started": "2023-05-11T07:04:09.105722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 23607)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfupdate = data_train.sample(frac=.15, replace=False, random_state=42)\n",
    "dfupdate.Word = 'UNKNOWN'\n",
    "data_train.update(dfupdate)\n",
    "words = list(set(data_train.Word.values))\n",
    "# Convert words and tags into numbers\n",
    "word2id = {w: i for i, w in enumerate(words)}\n",
    "tag2id = {t: i for i, t in enumerate(tags)}\n",
    "id2tag = {i: t for i, t in enumerate(tags)}\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:04:13.198087Z",
     "iopub.status.busy": "2023-05-11T07:04:13.197016Z",
     "iopub.status.idle": "2023-05-11T07:04:15.536434Z",
     "shell.execute_reply": "2023-05-11T07:04:15.535437Z",
     "shell.execute_reply.started": "2023-05-11T07:04:13.198047Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 702936/702936 [00:00<00:00, 953650.94it/s]\n"
     ]
    }
   ],
   "source": [
    "count_tags = dict(data_train.POS.value_counts())  # Total number of POS tags in the dataset\n",
    "# Now let's create the tags to words count\n",
    "count_tags_to_words = data_train.groupby(['POS']).apply(\n",
    "    lambda grp: grp.groupby('Word')['POS'].count().to_dict()).to_dict()\n",
    "# We shall also collect the counts for the first tags in the sentence\n",
    "count_init_tags = dict(data_train.groupby('sentence').first().POS.value_counts())\n",
    "\n",
    "# Create a mapping that stores the frequency of transitions in tags to it's next tags\n",
    "count_tags_to_next_tags = np.zeros((len(tags), len(tags)), dtype=int)\n",
    "sentences = list(data_train.sentence)\n",
    "pos = list(data_train.POS)\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        prevtagid = tag2id[pos[i - 1]]\n",
    "        nexttagid = tag2id[pos[i]]\n",
    "        count_tags_to_next_tags[prevtagid][nexttagid] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's build the parameter matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:04:16.628926Z",
     "iopub.status.busy": "2023-05-11T07:04:16.627946Z",
     "iopub.status.idle": "2023-05-11T07:04:17.564147Z",
     "shell.execute_reply": "2023-05-11T07:04:17.563118Z",
     "shell.execute_reply.started": "2023-05-11T07:04:16.628882Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [00:00<00:00, 69.28it/s]\n"
     ]
    }
   ],
   "source": [
    "startprob = np.zeros((len(tags),))\n",
    "transmat = np.zeros((len(tags), len(tags)))\n",
    "emissionprob = np.zeros((len(tags), len(words)))\n",
    "num_sentences = sum(count_init_tags.values())\n",
    "sum_tags_to_next_tags = np.sum(count_tags_to_next_tags, axis=1)\n",
    "for tag, tagid in tqdm(tag2id.items(), position=0, leave=True):\n",
    "    floatCountTag = float(count_tags.get(tag, 0))\n",
    "    startprob[tagid] = count_init_tags.get(tag, 0) / num_sentences\n",
    "    for word, wordid in word2id.items():\n",
    "        emissionprob[tagid][wordid] = count_tags_to_words.get(tag, {}).get(word, 0) / floatCountTag\n",
    "    for tag2, tagid2 in tag2id.items():\n",
    "        transmat[tagid][tagid2] = count_tags_to_next_tags[tagid][tagid2] / sum_tags_to_next_tags[tagid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:04:18.872065Z",
     "iopub.status.busy": "2023-05-11T07:04:18.871647Z",
     "iopub.status.idle": "2023-05-11T07:13:55.251430Z",
     "shell.execute_reply": "2023-05-11T07:13:55.250436Z",
     "shell.execute_reply.started": "2023-05-11T07:04:18.872028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23608, 23608)\n"
     ]
    }
   ],
   "source": [
    "#to create word transition matrix\n",
    "\n",
    "#first step is to count the number of times each word appears in the dataset\n",
    "count_words = {}\n",
    "for word in data_train.Word.values:\n",
    "    count_words[word] = count_words.get(word, 0) + 1\n",
    "\n",
    "# then count the number of times a word appears after another word\n",
    "count_word_transitions = {}\n",
    "for sentence in data_train.groupby('sentence'):\n",
    "    words = sentence[1]['Word'].values\n",
    "    for i in range(len(words) - 1):\n",
    "        w1, w2 = words[i], words[i+1]\n",
    "        if w1 not in count_word_transitions:\n",
    "            count_word_transitions[w1] = {}\n",
    "        count_word_transitions[w1][w2] = count_word_transitions[w1].get(w2, 0) + 1\n",
    "\n",
    "# convert the counts to probabilities\n",
    "word_transition_matrix = np.zeros((len(word2id)+1, len(word2id)+1))\n",
    "sum_words_to_next_words = np.sum([count_word_transitions[w1][w2] for w1 in count_word_transitions for w2 in count_word_transitions[w1]])\n",
    "for w1, w1id in word2id.items():\n",
    "    for w2, w2id in word2id.items():\n",
    "        word_transition_matrix[w1id][w2id] = count_word_transitions.get(w1, {}).get(w2, 0) / sum_words_to_next_words\n",
    "print(word_transition_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:15:49.510224Z",
     "iopub.status.busy": "2023-05-11T07:15:49.509838Z",
     "iopub.status.idle": "2023-05-11T07:15:49.516846Z",
     "shell.execute_reply": "2023-05-11T07:15:49.515890Z",
     "shell.execute_reply.started": "2023-05-11T07:15:49.510194Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_log_likelihood(sentence: List[str], word_transition_matrix) -> float:\n",
    "    \"\"\"\n",
    "    Given a sentence and word_transition_matrix, returns the log-likelihood of the sentence.\n",
    "    \"\"\"\n",
    "    # converting the sentence to a list of word IDs\n",
    "    sentence_ids = [word2id.get(w, word2id['UNKNOWN']) for w in sentence]\n",
    "\n",
    "    # calculating the log-likelihood using the word transition matrix\n",
    "    log_likelihood = np.log(word_transition_matrix[sentence_ids[0]][sentence_ids[1]])\n",
    "    for i in range(1, len(sentence_ids) - 1):\n",
    "        log_likelihood += np.log(word_transition_matrix[sentence_ids[i]][sentence_ids[i+1]] + 1e-10)\n",
    "    return log_likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:15:50.616988Z",
     "iopub.status.busy": "2023-05-11T07:15:50.616641Z",
     "iopub.status.idle": "2023-05-11T07:15:50.623438Z",
     "shell.execute_reply": "2023-05-11T07:15:50.622528Z",
     "shell.execute_reply.started": "2023-05-11T07:15:50.616960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-41.259970813020175"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_log_likelihood([\"This\", \"is\", \"a\", \"test\", \"sentence\"], word_transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will continue to constructing the HMM.\n",
    "\n",
    "We will use the hmmlearn implementation to initialize the HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:15:53.620109Z",
     "iopub.status.busy": "2023-05-11T07:15:53.619714Z",
     "iopub.status.idle": "2023-05-11T07:15:53.626301Z",
     "shell.execute_reply": "2023-05-11T07:15:53.624047Z",
     "shell.execute_reply.started": "2023-05-11T07:15:53.620074Z"
    }
   },
   "outputs": [],
   "source": [
    "model = hmm.MultinomialHMM(n_components=len(tags), algorithm='viterbi', random_state=42)\n",
    "model.startprob_ = startprob\n",
    "model.transmat_ = transmat\n",
    "model.emissionprob_ = emissionprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before using the HMM to predict the POS tags, we have to fix the training set as some of the words and tags in the test data might not appear in the training data so we collect this data to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:15:55.414039Z",
     "iopub.status.busy": "2023-05-11T07:15:55.413377Z",
     "iopub.status.idle": "2023-05-11T07:15:56.492187Z",
     "shell.execute_reply": "2023-05-11T07:15:56.491150Z",
     "shell.execute_reply.started": "2023-05-11T07:15:55.413982Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 345639/345639 [00:00<00:00, 2459582.87it/s]\n"
     ]
    }
   ],
   "source": [
    "data_test.loc[~data_test['Word'].isin(words), 'Word'] = 'UNKNOWN'\n",
    "word_test = list(data_test.Word)\n",
    "samples = []\n",
    "for i, val in enumerate(word_test):\n",
    "    samples.append([word2id[val]])\n",
    "\n",
    "# TODO use panda solution\n",
    "lengths = []\n",
    "count = 0\n",
    "sentences = list(data_test.sentence)\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        count += 1\n",
    "    elif i > 0:\n",
    "        lengths.append(count)\n",
    "        count = 1\n",
    "    else:\n",
    "        count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the HMM ready lets predict the best path from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:15:57.463354Z",
     "iopub.status.busy": "2023-05-11T07:15:57.462670Z",
     "iopub.status.idle": "2023-05-11T07:17:35.308851Z",
     "shell.execute_reply": "2023-05-11T07:17:35.307935Z",
     "shell.execute_reply.started": "2023-05-11T07:15:57.463302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1, 38, ..., 18, 24, 11])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_predict = model.predict(samples, lengths)\n",
    "pos_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:17:39.305271Z",
     "iopub.status.busy": "2023-05-11T07:17:39.304909Z",
     "iopub.status.idle": "2023-05-11T07:17:39.315290Z",
     "shell.execute_reply": "2023-05-11T07:17:39.314231Z",
     "shell.execute_reply.started": "2023-05-11T07:17:39.305243Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def viterbi(pi: np.array, a: np.array, b: np.array, obs: List) -> np.array:\n",
    "    \"\"\"\n",
    "     Write the viterbi algorithm from scratch to find the best probable path\n",
    "     attr:\n",
    "       pi: initial probabilities\n",
    "       a: transition probabilities\n",
    "       b: emission probabilities\n",
    "       obs: list of observations\n",
    "     return:\n",
    "       array of the indices of the best hidden states\n",
    "    \"\"\"\n",
    "    # state space cardinality\n",
    "    K = a.shape[0]\n",
    "\n",
    "    # observation sequence length\n",
    "    T = len(obs)\n",
    "\n",
    "    # initializing the tracking tables from first observation\n",
    "    delta = np.zeros((T, K))\n",
    "    psi = np.zeros((T, K))\n",
    "    delta[0] = pi * b[:, obs[0]]\n",
    "\n",
    "    # iterating throught the observations updating the tracking tables\n",
    "    for t in range(1, T):\n",
    "        for j in range(K):\n",
    "            delta[t, j] = np.max(delta[t-1] * a[:, j] * b[j, obs[t]])\n",
    "            psi[t, j] = np.argmax(delta[t-1] * a[:, j])\n",
    "\n",
    "    # build the output, optimal model trajectory\n",
    "    x = np.zeros(T, dtype=int)\n",
    "    x[T-1] = np.argmax(delta[T-1])\n",
    "    for t in range(T-2, -1, -1):\n",
    "        x[t] = psi[t+1, x[t+1]]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:17:44.477886Z",
     "iopub.status.busy": "2023-05-11T07:17:44.477187Z",
     "iopub.status.idle": "2023-05-11T07:17:44.482961Z",
     "shell.execute_reply": "2023-05-11T07:17:44.482064Z",
     "shell.execute_reply.started": "2023-05-11T07:17:44.477849Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "hidden_states = ['Sunny', 'Cloudy', 'Rainy']\n",
    "observable_states = ['Hot', 'Mild', 'Cold', 'Windy', 'Foggy']\n",
    "observations =  []\n",
    "\n",
    "for i in range(40):\n",
    "  obs_index = random.randint(0, len(observable_states)-1) # random index corresponding to the observable state\n",
    "  observations.append(obs_index) # then adding the index to the observations list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:17:45.363703Z",
     "iopub.status.busy": "2023-05-11T07:17:45.362846Z",
     "iopub.status.idle": "2023-05-11T07:17:45.417928Z",
     "shell.execute_reply": "2023-05-11T07:17:45.416981Z",
     "shell.execute_reply.started": "2023-05-11T07:17:45.363666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: [0, 1, 0, 3, 0, 1, 3, 2, 3, 1, 4, 0, 0, 4, 2, 4, 0, 2, 3, 3, 0, 0, 2, 1, 2, 1, 0, 0, 0, 3, 0, 3, 3, 0, 2, 3, 3, 4, 1, 1]\n",
      "Viterbi sequence: [25  0 40 35 40  0 35 28 35  0 35 40 25 35 28 35 27  2 35 35 40  3 28  0\n",
      "  2  0 15 38 25 35 40 35 35 27  2 35 35 35  0  0]\n"
     ]
    }
   ],
   "source": [
    "hidden_state_sequence = viterbi(startprob, transmat, emissionprob, observations)\n",
    "\n",
    "print(\"Observations:\", observations)\n",
    "print(\"Viterbi sequence:\", hidden_state_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:17:46.928906Z",
     "iopub.status.busy": "2023-05-11T07:17:46.928438Z",
     "iopub.status.idle": "2023-05-11T07:17:57.099730Z",
     "shell.execute_reply": "2023-05-11T07:17:57.098728Z",
     "shell.execute_reply.started": "2023-05-11T07:17:46.928866Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 405.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def baum_welch(observations, observations_vocab, n_hidden_states):\n",
    "    \"\"\"\n",
    "    Baum-Welch algorithm for estimating the HMM parameters\n",
    "    :param observations: observations\n",
    "    :param observations_vocab: observations vocabulary\n",
    "    :param n_hidden_states: number of hidden states to estimate\n",
    "    :return: a, b (transition matrix and emission matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward_probs(observations, observations_vocab, n_hidden_states, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "        forward pass to calculate alpha\n",
    "        :param observations: observations\n",
    "        :param observations_vocab: observation vocabulary\n",
    "        :param n_hidden_states: number of hidden states\n",
    "        :param a_: estimated alpha\n",
    "        :param b_: estimated beta\n",
    "        :return: refined alpha_\n",
    "        \"\"\"\n",
    "        a_start = 1 / n_hidden_states\n",
    "        alpha_ = np.zeros((n_hidden_states, len(observations)), dtype=float)\n",
    "        alpha_[:, 0] = a_start\n",
    "        for t in range(1, len(observations)):\n",
    "          for j in range(n_hidden_states):\n",
    "            calc = observations_vocab == observations[t]\n",
    "            for i in range(n_hidden_states):\n",
    "              alpha_[j, t] = sum(alpha_[i, t-1]*a_[i,j] * b_[j, np.where(calc)[0][0]] for i in range(n_hidden_states))\n",
    "\n",
    "        return alpha_\n",
    "\n",
    "    def backward_probs(observations, observations_vocab, n_hidden_states, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "        backward pass to calculate alpha\n",
    "        :param observations: observations\n",
    "        :param observations_vocab: observation vocabulary\n",
    "        :param n_hidden_states: number of hidden states\n",
    "        :param a_: estimated alpha\n",
    "        :param b_: estimated beta\n",
    "        :return: refined beta_\n",
    "        \"\"\"\n",
    "        beta_ = np.zeros((n_hidden_states, len(observations)), dtype=float)\n",
    "        beta_[:, -1:] = 1\n",
    "        for t in range(len(observations) -2, -1, -1):\n",
    "          for i in range(n_hidden_states):\n",
    "            calc2 = observations_vocab == observations[t+1]\n",
    "            beta_[i,t] = sum(a_[i,j] * b_[j, np.where(calc2)[0][0]]*beta_[j, t+1] for j in range(n_hidden_states))\n",
    "        return beta_\n",
    "\n",
    "    def compute_gamma(alfa, beta, observations, vocab, n_samples, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "\n",
    "        :param alfa:\n",
    "        :param beta:\n",
    "        :param observations:\n",
    "        :param vocab:\n",
    "        :param n_samples:\n",
    "        :param a_:\n",
    "        :param b_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # gamma_prob = np.zeros(n_samples, len(observations))\n",
    "        gamma_prob = np.multiply(alfa, beta) / sum(np.multiply(alfa, beta))\n",
    "        return gamma_prob\n",
    "\n",
    "    def compute_sigma(alfa, beta, observations, vocab, n_samples, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "\n",
    "        :param alfa:\n",
    "        :param beta:\n",
    "        :param observations:\n",
    "        :param vocab:\n",
    "        :param n_samples:\n",
    "        :param a_:\n",
    "        :param b_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sigma_prob = np.zeros((n_samples, len(observations) - 1, n_samples), dtype=float)\n",
    "        denomenator = np.multiply(alfa, beta)\n",
    "        for i in range(len(observations) - 1):\n",
    "            for j in range(n_samples):\n",
    "                for k in range(n_samples):\n",
    "                    index_in_vocab = np.where(vocab == observations[i + 1])[0][0]\n",
    "                    sigma_prob[j, i, k] = (alfa[j, i] * beta[k, i + 1] * a_[j, k] * b_[k, index_in_vocab]) / sum(\n",
    "                        denomenator[:, j])\n",
    "        return sigma_prob\n",
    "\n",
    "    # initialize A ,B\n",
    "    a = np.ones((n_hidden_states, n_hidden_states)) / n_hidden_states\n",
    "    b = np.ones((n_hidden_states, len(observations_vocab))) / len(observations_vocab)\n",
    "    for iter in tqdm(range(2000), position=0, leave=True):\n",
    "\n",
    "        # E-step caclculating sigma and gamma\n",
    "        alfa_prob = forward_probs(observations, observations_vocab, n_hidden_states, a, b)  #\n",
    "        beta_prob = backward_probs(observations, observations_vocab, n_hidden_states, a, b)  # , beta_val\n",
    "        gamma_prob = compute_gamma(alfa_prob, beta_prob, observations, observations_vocab, n_hidden_states, a, b)\n",
    "        sigma_prob = compute_sigma(alfa_prob, beta_prob, observations, observations_vocab, n_hidden_states, a, b)\n",
    "\n",
    "        # M-step caclculating A, B matrices\n",
    "        a_model = np.zeros((n_hidden_states, n_hidden_states))\n",
    "        for j in range(n_hidden_states):  # calculate A-model\n",
    "            for i in range(n_hidden_states):\n",
    "                for t in range(len(observations) - 1):\n",
    "                    a_model[j, i] = a_model[j, i] + sigma_prob[j, t, i]\n",
    "                normalize_a = [sigma_prob[j, t_current, i_current] for t_current in range(len(observations) - 1) for\n",
    "                               i_current in range(n_hidden_states)]\n",
    "                normalize_a = sum(normalize_a)\n",
    "                if normalize_a == 0:\n",
    "                    a_model[j, i] = 0\n",
    "                else:\n",
    "                    a_model[j, i] = a_model[j, i] / normalize_a\n",
    "\n",
    "        b_model = np.zeros((n_hidden_states, len(observations_vocab)))\n",
    "\n",
    "        for j in range(n_hidden_states):\n",
    "            for i in range(len(observations_vocab)):\n",
    "                indices = [idx for idx, val in enumerate(observations) if val == observations_vocab[i]]\n",
    "                numerator_b = sum(gamma_prob[j, indices])\n",
    "                denominator_b = sum(gamma_prob[j, :])\n",
    "                if denominator_b == 0:\n",
    "                    b_model[j, i] = 0\n",
    "                else:\n",
    "                    b_model[j, i] = numerator_b / denominator_b\n",
    "\n",
    "        a = a_model\n",
    "        b = b_model\n",
    "    return a, b\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "hidden_states = ['healthy', 'sick']\n",
    "observable_states = ['sleeping', 'eating', 'pooping']\n",
    "observable_map = {'sleeping': 0, 'eating': 1, 'pooping': 2}\n",
    "observations = []\n",
    "for i in range(40):\n",
    "    observations.append(observable_map[random.choice(observable_states)])\n",
    "\n",
    "A, B = baum_welch(observations=observations, observations_vocab=np.array(list(observable_map.values())),\n",
    "                  n_hidden_states=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented the Baum-Welch algorithm for estimating the HMM parameters by the forward, backward, and E-step functions to compute the gamma and sigma probabilities. These probabilities are used to estimate the HMM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T07:17:57.102332Z",
     "iopub.status.busy": "2023-05-11T07:17:57.101959Z",
     "iopub.status.idle": "2023-05-11T07:17:57.158586Z",
     "shell.execute_reply": "2023-05-11T07:17:57.157659Z",
     "shell.execute_reply.started": "2023-05-11T07:17:57.102298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: [1, 0, 0, 2, 1, 1, 0, 0, 0, 2, 1, 1, 1, 2, 2, 0, 2, 2, 0, 2, 1, 1, 2, 2, 0, 2, 0, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Viterbi sequence: [ 0 40  3 28  0  0 40  3  9  2  0  0  0  2  2  3 28  2  3 28  0  0  2  2\n",
      "  3 28  3 28  2  2  0  0  0  0 15 38 25  0 19  0]\n"
     ]
    }
   ],
   "source": [
    "#TASK 4: Now try it with your HMM\n",
    "hidden_state_sequence = viterbi(startprob, transmat, emissionprob, observations)\n",
    "\n",
    "print(\"Observations:\", observations)\n",
    "print(\"Viterbi sequence:\", hidden_state_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the goal of this project is to demonstrate the effectiveness of HMM and POS tagging in predicting the likelihood of a given sentence and to compare and contrast different approaches to achieving this goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
